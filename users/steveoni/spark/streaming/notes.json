[{"grab":"\n\n**foreachBatch()**\n\n```py\n hostAddr = \"<ip address>\"\nkeyspaceName = \"<keyspace>\"\ntableName = \"<tableName>\"\nspark.conf.set(\"spark.cassandra.connection.host\", hostAddr)\ndef writeCountsToCassandra(updatedCountsDF, batchId):\n # Use Cassandra batch data source to write the updated counts\n (updatedCountsDF\n .write\n .format(\"org.apache.spark.sql.cassandra\")\n .mode(\"append\")\n .options(table=tableName, keyspace=keyspaceName)\n .save())\n\nstreamingQuery = (counts\n .writeStream\n .foreachBatch(writeCountsToCassandra)\n .outputMode(\"update\")\n .option(\"checkpointLocation\", checkpointDir)\n .start()) \n```\n ","views":"**foreachBatch()** takes in function that accept Dataframe output and batchid for each microbatch output","tags":["streaming"]}]