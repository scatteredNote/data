[{"grab":"\n\n**foreachBatch()**\n\n```py\n hostAddr = \"<ip address>\"\nkeyspaceName = \"<keyspace>\"\ntableName = \"<tableName>\"\nspark.conf.set(\"spark.cassandra.connection.host\", hostAddr)\ndef writeCountsToCassandra(updatedCountsDF, batchId):\n # Use Cassandra batch data source to write the updated counts\n (updatedCountsDF\n .write\n .format(\"org.apache.spark.sql.cassandra\")\n .mode(\"append\")\n .options(table=tableName, keyspace=keyspaceName)\n .save())\n\nstreamingQuery = (counts\n .writeStream\n .foreachBatch(writeCountsToCassandra)\n .outputMode(\"update\")\n .option(\"checkpointLocation\", checkpointDir)\n .start()) \n```\n ","views":"**foreachBatch()** takes in function that accept Dataframe output and batchid for each microbatch output","tags":["streaming"]},{"grab":"```py\n def writeCountsToMultipleLocations(updatedCountsDF, batchId):\n updatedCountsDF.persist()\n updatedCountsDF.write.format(...).save() # Location 1\n updatedCountsDF.write.format(...).save() # Location 2\n updatedCountsDF.unpersist() \n```\n ","views":"Each attempt to write might casuses the output data to be recomputed. including rereading of data. To prevent this we need to persist the output dataframe  by caching \n\nusing **outputDf.persist()** and **outputDf.unpersist()**\n","tags":["streaming"]},{"grab":"```py\ndef process_row(row):\n # Write row to storage\n pass\nquery = streamingDF.writeStream.foreach(process_row).start()\n# Variation 2: Using the ForeachWriter class\nclass ForeachWriter:\n def open(self, partitionId, epochId):\n # Open connection to data store\n # Return True if write should continue\n # This method is optional in Python\n # If not specified, the write will continue automatically\n return True\n def process(self, row):\n # Write string to data store using opened connection\n # This method is NOT optional in Python\n pass\n def close(self, error):\n # Close the connection. This method is optional in Python\n pass\nresultDF.writeStream.foreach(ForeachWriter()).start()\n```\n","views":"if not option to use `foreachBatch()` i.e if the batch data writer does not exist for the sink you can use `foreach`","tags":["streaming"]},{"grab":"## Stateless Transformations\nAll projection operations (e.g., select(), explode(), map(), flatMap()) and selec‚Äê\ntion operations (e.g., filter(), where()) process each input record individually\nwithout needing any information from previous rows. This lack of dependence on\nprior input data makes them stateless operations.","views":"This operation does not depend on the previous batch data and do make use of `append` and `update` to add data to the sink.","tags":["streaming"]}]