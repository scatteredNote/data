[{"grab":"\n\n**foreachBatch()**\n\n```py\n hostAddr = \"<ip address>\"\nkeyspaceName = \"<keyspace>\"\ntableName = \"<tableName>\"\nspark.conf.set(\"spark.cassandra.connection.host\", hostAddr)\ndef writeCountsToCassandra(updatedCountsDF, batchId):\n # Use Cassandra batch data source to write the updated counts\n (updatedCountsDF\n .write\n .format(\"org.apache.spark.sql.cassandra\")\n .mode(\"append\")\n .options(table=tableName, keyspace=keyspaceName)\n .save())\n\nstreamingQuery = (counts\n .writeStream\n .foreachBatch(writeCountsToCassandra)\n .outputMode(\"update\")\n .option(\"checkpointLocation\", checkpointDir)\n .start()) \n```\n ","views":"**foreachBatch()** takes in function that accept Dataframe output and batchid for each microbatch output","tags":["streaming"]},{"grab":"```py\n def writeCountsToMultipleLocations(updatedCountsDF, batchId):\n updatedCountsDF.persist()\n updatedCountsDF.write.format(...).save() # Location 1\n updatedCountsDF.write.format(...).save() # Location 2\n updatedCountsDF.unpersist() \n```\n ","views":"Each attempt to write might casuses the output data to be recomputed. including rereading of data. To prevent this we need to persist the output dataframe  by caching \n\nusing **outputDf.persist()** and **outputDf.unpersist()**\n","tags":["streaming"]}]