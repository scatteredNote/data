[{"grab":"\n\n**foreachBatch()**\n\n```py\n hostAddr = \"<ip address>\"\nkeyspaceName = \"<keyspace>\"\ntableName = \"<tableName>\"\nspark.conf.set(\"spark.cassandra.connection.host\", hostAddr)\ndef writeCountsToCassandra(updatedCountsDF, batchId):\n # Use Cassandra batch data source to write the updated counts\n (updatedCountsDF\n .write\n .format(\"org.apache.spark.sql.cassandra\")\n .mode(\"append\")\n .options(table=tableName, keyspace=keyspaceName)\n .save())\n\nstreamingQuery = (counts\n .writeStream\n .foreachBatch(writeCountsToCassandra)\n .outputMode(\"update\")\n .option(\"checkpointLocation\", checkpointDir)\n .start()) \n```\n ","views":"**foreachBatch()** takes in function that accept Dataframe output and batchid for each microbatch output","tags":["streaming"]},{"grab":"```py\n def writeCountsToMultipleLocations(updatedCountsDF, batchId):\n updatedCountsDF.persist()\n updatedCountsDF.write.format(...).save() # Location 1\n updatedCountsDF.write.format(...).save() # Location 2\n updatedCountsDF.unpersist() \n```\n ","views":"Each attempt to write might casuses the output data to be recomputed. including rereading of data. To prevent this we need to persist the output dataframe  by caching \n\nusing **outputDf.persist()** and **outputDf.unpersist()**\n","tags":["streaming"]},{"grab":"```py\ndef process_row(row):\n # Write row to storage\n pass\nquery = streamingDF.writeStream.foreach(process_row).start()\n# Variation 2: Using the ForeachWriter class\nclass ForeachWriter:\n def open(self, partitionId, epochId):\n # Open connection to data store\n # Return True if write should continue\n # This method is optional in Python\n # If not specified, the write will continue automatically\n return True\n def process(self, row):\n # Write string to data store using opened connection\n # This method is NOT optional in Python\n pass\n def close(self, error):\n # Close the connection. This method is optional in Python\n pass\nresultDF.writeStream.foreach(ForeachWriter()).start()\n```\n","views":"if not option to use `foreachBatch()` i.e if the batch data writer does not exist for the sink you can use `foreach`","tags":["streaming"]},{"grab":"## Stateless Transformations\nAll projection operations (e.g., select(), explode(), map(), flatMap()) and selec‚Äê\ntion operations (e.g., filter(), where()) process each input record individually\nwithout needing any information from previous rows. This lack of dependence on\nprior input data makes them stateless operations.","views":"This operation does not depend on the previous batch data and do make use of `append` and `update` to add data to the sink.","tags":["streaming"]},{"grab":"## Stateful Operation","views":"For stateful operation the executor save batch data in in-memory for the next data.\n\nThis does not ensure no failure. to resolve that, each key/value state update a change log in checkpoint. it is version along with the offset range.","tags":["streaming"]},{"grab":"## StateFuL Streaming Aggregation\n\nAggregation not base on time\n\nGlobal Aggregation\n\n```py\nrunningCount = sensorReadings.groupBy().count()\n```\n\n**Grouped Aggregations**\n\n```py\nbaselineValues = sensorReadings.groupBy(\"sensorId\").mean(\"value\")\n```\n\n","views":"for global aggregation `df.groupBy().count()`\n\nwe can't run `df.count()` directly, because for static dataframes this return the aggregate immediately, whereas for stream the aggragate is meant to be calculated continously","tags":["streaming"]},{"grab":"## Aggregations with Event-Time Windows\n\n```py\nfrom pyspark.sql.functions import *\n(sensorReadings\n .groupBy(\"sensorId\", window(\"eventTime\", \"5 minute\"))\n .count())\n```\n\n```py\n# In Python\n(sensorReadings\n .groupBy(\"sensorId\", window(\"eventTime\", \"10 minute\", \"5 minute\"))\n .count())\n```","views":"while dealing with time data like sensor it best to calculate aggreagte base on the time in the data (collected by the sensor) and no base on the time the data enters the streams\n\nslidding window do have overlapped data. for the example. taked data from the input at range of 10minutes and move the window my 5 minute data.\n\n```\nFor example, the event\n[eventTime = 12:07, sensorId = id1] gets mapped to two time windows and\ntherefore two groups, (12:00-12:10, id1) and (12:05-12:15, id1). The counts for\nthese two windows are each incremented by 1\n```","tags":["streaming"]},{"grab":"## supported output modes for aggregation with time","views":"**update mode**: support all type of aggragagtion. but can not be use with append-only streaming sink.\n\n**complete mode**:  support all aggragation. but for window time aggragation, this mode will preserve all state instead of them to be cleanup if watermark is specified. this might increase the data size and memory usage\n\n**append mode**: good for append-only streaming sink. only disadvantage is that the output will be delayed by watermark duration. i.e query has to wait for trailing watermark to exceed the time window of a key feature before its aggregate can be finalized","tags":["streaming"]}]