[{"grab":"## Edit/Create/modify Warehouse via sql worksheet interface","views":"```sql\nCREATE WAREHOUSE \"TEST_WH\" \nWITH WAREHOUSE_SIZE = 'SMALL' \nAUTO_SUSPEND = 600 \nAUTO_RESUME = TRUE \nMIN_CLUSTER_COUNT = 1 \nMAX_CLUSTER_COUNT = 2 \nSCALING_POLICY = 'STANDARD' \nCOMMENT = ' '\n```\n\nAlter warehouse\n\n```sql\nALTER WAREHOUSE \"TEST_WH\" \nSET WAREHOUSE_SIZE = 'SMALL' \nAUTO_SUSPEND = 1200 \nAUTO_RESUME = TRUE \nMIN_CLUSTER_COUNT = 1 \nMAX_CLUSTER_COUNT = 1 \nSCALING_POLICY = 'STANDARD' \nCOMMENT = ' '\n```\n\n### View warehouses\n\n```sql\nSHOW WAREHOUSES\n```\n\n```sql\nALTER WAREHOUSE TEST_WH SUSPEND\n```\n\n```sql\n\nALTER WAREHOUSE \"TEST_WH\" RESUME If SUSPENDED\n\n```\n\n### Drop warehouse\n\n```sql\n\nDROP WAREHOUSE \"TEST_WH\"\n```","tags":["snowflakes"]},{"grab":"## Creating DB\n\nDatabase can be created by going to  the `Data` section and click on `+ Database`.\n\nand via the ui you can clone local db and drop any db.\n\nAlso this can be done via the worksheet sql","views":"```sql\ncreate DATABASE \"TEST_DB_2\"\n\nSHOW DATABASES\n\nCREATE DATABASE TEST_DB_3 CLONE \"TEST_DB_2\"\n\nDROP DATABASE \"TEST_DB_2\"\n```","tags":["snowflakes"]},{"grab":"## creatin schema for database via sql worksheets","views":"```sql\nCREATE SCHEMA \"TEST_DB\".\"TEST_SCHEMA\"\n\n```\n\nwhere `TEST_SCHEMA` is schema created under `TEST_DB`\n\nedit the schema name\n\n```sql\n\nALTER SCHEMA \"TEST_DB\".\"TEST_SCHEMA\" RENAME TO \"TEST_DB\".\"TEST_SCHEMA_RENAME\"\n\n```\n\n```sql\n\nSHOW SCHEMAS\n\nCREATE SCHEMA \"TEST_DB\".\"TEST2\" CLONE \"TEST_DB\".\"TEST_SCHEMA_RENAME\"\n\nDROP SCHEMA \"TEST_DB\".\"TEST2\"\n```","tags":["snowflakes"]},{"grab":"assign warehouse creation to a role","views":"```sql\nGRANT CREATE WAREHOUSE ON ACCOUNT TO ROLE kafka_connector_role_1;\n```","tags":[]},{"grab":"Enable kafka to write to snowflake\n","views":"create a db and schema\n\ngo to worksheet and do the following\n\n* Add public key to the user on snowflake\n\n```sql\nALTER USER STEVEONI set rsa_public_key_2=\"\n```\ncreate a role and give it access to table schema , table, stage and pipe\n\n```sql\nUSE ROLE accountadmin;\n\n-- Create a Snowflake role with the privileges to work with the connector.\nCREATE ROLE kafka_connector_role_1;\n\nCREATE DATABASE kafka_db\n-- Grant privileges on the database.\nGRANT USAGE ON DATABASE kafka_db TO ROLE kafka_connector_role_1;\n\n-- Grant privileges on the schema.\nGRANT USAGE ON SCHEMA KAFKA_SCHEMA TO ROLE kafka_connector_role_1;\nGRANT CREATE TABLE ON SCHEMA kafka_schema TO ROLE kafka_connector_role_1;\nGRANT CREATE STAGE ON SCHEMA kafka_schema TO ROLE kafka_connector_role_1;\nGRANT CREATE PIPE ON SCHEMA kafka_schema TO ROLE kafka_connector_role_1;\n\nGRANT ROLE kafka_connector_role_1 TO USER STEVEONI;\n\nGRANT ROLE kafka_connector_role_1 TO USER STEVEONI;\n```\n\nGive permission to the new role to create a warehouse.\n\nthen create a warehoues\n\n","tags":[]},{"grab":"## Snowflakes object and funtionalities","views":" overview of various Snowflake concepts, their purpose, and examples:\n\n1. **Views**:\n   - **Overview**: Views are virtual tables created by executing a SQL query on one or more base tables. They are not materialized and don't store data themselves but provide a convenient way to access and transform data.\n   - **Purpose**: To simplify complex queries, control access to data, and abstract underlying table structures.\n   - **Example**:\n     ```sql\n     CREATE VIEW sales_2022 AS\n     SELECT product_name, SUM(sales_amount) AS total_sales\n     FROM sales\n     WHERE sales_date BETWEEN '2022-01-01' AND '2022-12-31'\n     GROUP BY product_name;\n     ```\n\n2. **Stage**:\n   - **Overview**: A stage is an intermediate storage location used to load and unload data between Snowflake and external storage (e.g., AWS S3, Azure Blob Storage).\n   - **Purpose**: Efficiently move data in and out of Snowflake while leveraging cloud-based storage.\n   - **Example**:\n     ```sql\n     CREATE OR REPLACE STAGE my_stage\n     URL = 's3://my-bucket/my-path/'\n     CREDENTIALS = (AWS_KEY_ID = 'your-key-id' AWS_SECRET_KEY = 'your-secret-key');\n     ```\n\n3. **Pipe**:\n   - **Overview**: A pipe is a Snowflake object used to automate the ingestion of data from an external stage into a Snowflake table. It's typically associated with Snowflake Streams for continuous data loading.\n   - **Purpose**: Automate data loading processes and provide a consistent way to ingest data.\n   - **Example**:\n     ```sql\n     CREATE OR REPLACE PIPE my_pipe\n     AUTO_INGEST = TRUE\n     AS\n     COPY INTO my_table FROM '@my_stage/'\n     FILE_FORMAT = (TYPE = CSV);\n     ```\n\n4. **Storage Integration**:\n   - **Overview**: A storage integration allows Snowflake to access external cloud storage services securely. It includes credentials and configurations.\n   - **Purpose**: Facilitate seamless data exchange between Snowflake and cloud storage providers.\n   - **Example**:\n     ```sql\n     CREATE STORAGE INTEGRATION my_integration\n     TYPE = EXTERNAL_STAGE\n     ENABLED = TRUE\n     STORAGE_PROVIDER = S3\n     CREDENTIALS = (AWS_ROLE = 'my-iam-role');\n     ```\n\n5. **File Format**:\n   - **Overview**: A file format defines how data is stored and organized within files stored in external stages. It includes details like file type, compression, and delimiter.\n   - **Purpose**: Specify how data should be read from or written to external files.\n   - **Example**:\n     ```sql\n     CREATE OR REPLACE FILE FORMAT my_format\n     TYPE = CSV\n     FIELD_OPTIONALLY_ENCLOSED_BY = ''\n     SKIP_HEADER = 1;\n     ```\n\n6. **Sequence**:\n   - **Overview**: A sequence is an object that generates unique numeric values, often used for creating surrogate keys.\n   - **Purpose**: Generate unique identifiers for rows in a table.\n   - **Example**:\n     ```sql\n     CREATE SEQUENCE my_sequence\n     START = 1\n     INCREMENT = 1;\n     ```\n\n7. **Stream**:\n   - **Overview**: A stream is a change tracking mechanism in Snowflake. It captures changes made to a table and can be used for CDC (Change Data Capture) and data replication.\n   - **Purpose**: Track and propagate changes in a table.\n   - **Example**:\n     ```sql\n     CREATE STREAM my_stream\n     ON TABLE my_table;\n     ```\n\n8. **Task**:\n   - **Overview**: A task is a scheduled or one-time execution of SQL statements. It can be used for automation, ETL, and routine maintenance.\n   - **Purpose**: Automate SQL execution on a schedule or event.\n   - **Example**:\n     ```sql\n     CREATE TASK my_task\n     WAREHOUSE = my_warehouse\n     SCHEDULE = '5 minute'\n     AS\n     INSERT INTO my_summary_table\n     SELECT ...\n     ```\n\n9. **Function** and **Procedure**:\n   - **Overview**: Functions are reusable SQL code blocks that return a value, while procedures are reusable code blocks that perform an action.\n   - **Purpose**: Encapsulate logic and promote code reuse.\n   - **Example (Function)**:\n     ```sql\n     CREATE OR REPLACE FUNCTION add_numbers(x INT, y INT)\n     RETURNS INT\n     AS\n     'x + y';\n     ```\n\n10. **Dynamic Table**:\n    - **Overview**: Dynamic tables are temporary tables created and dropped during a session. They're used for intermediate data processing.\n    - **Purpose**: Temporarily store data for complex operations without affecting permanent tables.\n    - **Example**:\n      ```sql\n      CREATE TEMPORARY TABLE temp_table AS\n      SELECT ...\n      ```\n\nThese are fundamental Snowflake concepts that cover various aspects of data storage, processing, and automation within the Snowflake data warehouse.","tags":["snowflakes"]},{"grab":"## staging using Amazon s3","views":"\n**Step-by-Step Operation:**\n\n**Step 1: Set Up Snowflake and Amazon S3**\n\nEnsure you have a Snowflake account and an Amazon S3 bucket set up. Replace placeholders in angle brackets (<>) with your actual values.\n\n**Step 2: Create a Stage in Snowflake**\n\nIn Snowflake, create an internal stage to specify the S3 bucket as the staging location:\n\n```sql\n-- Create a stage\nCREATE OR REPLACE STAGE my_stage\nURL = 's3://<your-s3-bucket>/<path-to-folder>'\nCREDENTIALS = (\n  AWS_KEY_ID = '<your-aws-access-key-id>'\n  AWS_SECRET_KEY = '<your-aws-secret-access-key>'\n);\n```\n\nThis step establishes a connection to your S3 bucket.\n\n**Step 3: Copy Data to Snowflake Stage**\n\nYou can copy data from your S3 bucket to the Snowflake stage using the `COPY INTO` command. This command specifies the source and target:\n\n```sql\n-- Copy data into Snowflake stage\nCOPY INTO my_stage\nFROM 's3://<your-s3-bucket>/<source-path>'\nFILES = ('file1.csv', 'file2.csv')\nFILE_FORMAT = (TYPE = CSV);\n```\n\nThis command copies data from the specified S3 path and files into your Snowflake stage.\n\n**Step 4: Perform Data Operations**\n\nYou can now perform various data operations on the staged data using SQL commands:\n\n```sql\n-- Example: Create a table and load data from the stage\nCREATE OR REPLACE TABLE my_table AS\nSELECT *\nFROM @my_stage/file1.csv;\n```\n\nHere, we create a new table in Snowflake and load data from the staged CSV file.\n\n**Step 5: Copy Data Back to S3 (Optional)**\n\nAfter processing the data, if you need to write the results back to S3, you can use the `COPY INTO` command again to copy data from Snowflake to S3:\n\n```sql\n-- Copy data from Snowflake to S3\nCOPY INTO 's3://<your-s3-bucket>/<destination-path>'\nFROM my_table\nFILE_FORMAT = (TYPE = CSV);\n```\n\nThis command exports data from Snowflake to the specified S3 location.\n\n**Step 6: Clean Up (Optional)**\n\nYou can optionally clean up the staged data and stage by dropping them:\n\n```sql\n-- Drop the stage\nDROP STAGE my_stage;\n\n-- Drop the table if it's no longer needed\nDROP TABLE my_table;\n```\nStaging allows you to efficiently transfer and process data between Snowflake and cloud storage services like S3 while maintaining data integrity and security.","tags":["snowflakes"]}]