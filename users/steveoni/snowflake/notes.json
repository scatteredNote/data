[{"grab":"## Edit/Create/modify Warehouse via sql worksheet interface","views":"```sql\nCREATE WAREHOUSE \"TEST_WH\" \nWITH WAREHOUSE_SIZE = 'SMALL' \nAUTO_SUSPEND = 600 \nAUTO_RESUME = TRUE \nMIN_CLUSTER_COUNT = 1 \nMAX_CLUSTER_COUNT = 2 \nSCALING_POLICY = 'STANDARD' \nCOMMENT = ' '\n```\n\nAlter warehouse\n\n```sql\nALTER WAREHOUSE \"TEST_WH\" \nSET WAREHOUSE_SIZE = 'SMALL' \nAUTO_SUSPEND = 1200 \nAUTO_RESUME = TRUE \nMIN_CLUSTER_COUNT = 1 \nMAX_CLUSTER_COUNT = 1 \nSCALING_POLICY = 'STANDARD' \nCOMMENT = ' '\n```\n\n### View warehouses\n\n```sql\nSHOW WAREHOUSES\n```\n\n```sql\nALTER WAREHOUSE TEST_WH SUSPEND\n```\n\n```sql\n\nALTER WAREHOUSE \"TEST_WH\" RESUME If SUSPENDED\n\n```\n\n### Drop warehouse\n\n```sql\n\nDROP WAREHOUSE \"TEST_WH\"\n```","tags":["snowflakes"]},{"grab":"## Creating DB\n\nDatabase can be created by going to  the `Data` section and click on `+ Database`.\n\nand via the ui you can clone local db and drop any db.\n\nAlso this can be done via the worksheet sql","views":"```sql\ncreate DATABASE \"TEST_DB_2\"\n\nSHOW DATABASES\n\nCREATE DATABASE TEST_DB_3 CLONE \"TEST_DB_2\"\n\nDROP DATABASE \"TEST_DB_2\"\n```","tags":["snowflakes"]},{"grab":"## creatin schema for database via sql worksheets","views":"```sql\nCREATE SCHEMA \"TEST_DB\".\"TEST_SCHEMA\"\n\n```\n\nwhere `TEST_SCHEMA` is schema created under `TEST_DB`\n\nedit the schema name\n\n```sql\n\nALTER SCHEMA \"TEST_DB\".\"TEST_SCHEMA\" RENAME TO \"TEST_DB\".\"TEST_SCHEMA_RENAME\"\n\n```\n\n```sql\n\nSHOW SCHEMAS\n\nCREATE SCHEMA \"TEST_DB\".\"TEST2\" CLONE \"TEST_DB\".\"TEST_SCHEMA_RENAME\"\n\nDROP SCHEMA \"TEST_DB\".\"TEST2\"\n```","tags":["snowflakes"]},{"grab":"assign warehouse creation to a role","views":"```sql\nGRANT CREATE WAREHOUSE ON ACCOUNT TO ROLE kafka_connector_role_1;\n```","tags":[]},{"grab":"Enable kafka to write to snowflake\n","views":"create a db and schema\n\ngo to worksheet and do the following\n\n* Add public key to the user on snowflake\n\n```sql\nALTER USER STEVEONI set rsa_public_key_2=\"\n```\ncreate a role and give it access to table schema , table, stage and pipe\n\n```sql\nUSE ROLE accountadmin;\n\n-- Create a Snowflake role with the privileges to work with the connector.\nCREATE ROLE kafka_connector_role_1;\n\nCREATE DATABASE kafka_db\n-- Grant privileges on the database.\nGRANT USAGE ON DATABASE kafka_db TO ROLE kafka_connector_role_1;\n\n-- Grant privileges on the schema.\nGRANT USAGE ON SCHEMA KAFKA_SCHEMA TO ROLE kafka_connector_role_1;\nGRANT CREATE TABLE ON SCHEMA kafka_schema TO ROLE kafka_connector_role_1;\nGRANT CREATE STAGE ON SCHEMA kafka_schema TO ROLE kafka_connector_role_1;\nGRANT CREATE PIPE ON SCHEMA kafka_schema TO ROLE kafka_connector_role_1;\n\nGRANT ROLE kafka_connector_role_1 TO USER STEVEONI;\n\nGRANT ROLE kafka_connector_role_1 TO USER STEVEONI;\n```\n\nGive permission to the new role to create a warehouse.\n\nthen create a warehoues\n\n","tags":[]},{"grab":"## Snowflakes object and funtionalities","views":" overview of various Snowflake concepts, their purpose, and examples:\n\n1. **Views**:\n   - **Overview**: Views are virtual tables created by executing a SQL query on one or more base tables. They are not materialized and don't store data themselves but provide a convenient way to access and transform data.\n   - **Purpose**: To simplify complex queries, control access to data, and abstract underlying table structures.\n   - **Example**:\n     ```sql\n     CREATE VIEW sales_2022 AS\n     SELECT product_name, SUM(sales_amount) AS total_sales\n     FROM sales\n     WHERE sales_date BETWEEN '2022-01-01' AND '2022-12-31'\n     GROUP BY product_name;\n     ```\n\n2. **Stage**:\n   - **Overview**: A stage is an intermediate storage location used to load and unload data between Snowflake and external storage (e.g., AWS S3, Azure Blob Storage).\n   - **Purpose**: Efficiently move data in and out of Snowflake while leveraging cloud-based storage.\n   - **Example**:\n     ```sql\n     CREATE OR REPLACE STAGE my_stage\n     URL = 's3://my-bucket/my-path/'\n     CREDENTIALS = (AWS_KEY_ID = 'your-key-id' AWS_SECRET_KEY = 'your-secret-key');\n     ```\n\n3. **Pipe**:\n   - **Overview**: A pipe is a Snowflake object used to automate the ingestion of data from an external stage into a Snowflake table. It's typically associated with Snowflake Streams for continuous data loading.\n   - **Purpose**: Automate data loading processes and provide a consistent way to ingest data.\n   - **Example**:\n     ```sql\n     CREATE OR REPLACE PIPE my_pipe\n     AUTO_INGEST = TRUE\n     AS\n     COPY INTO my_table FROM '@my_stage/'\n     FILE_FORMAT = (TYPE = CSV);\n     ```\n\n4. **Storage Integration**:\n   - **Overview**: A storage integration allows Snowflake to access external cloud storage services securely. It includes credentials and configurations.\n   - **Purpose**: Facilitate seamless data exchange between Snowflake and cloud storage providers.\n   - **Example**:\n     ```sql\n     CREATE STORAGE INTEGRATION my_integration\n     TYPE = EXTERNAL_STAGE\n     ENABLED = TRUE\n     STORAGE_PROVIDER = S3\n     CREDENTIALS = (AWS_ROLE = 'my-iam-role');\n     ```\n\n5. **File Format**:\n   - **Overview**: A file format defines how data is stored and organized within files stored in external stages. It includes details like file type, compression, and delimiter.\n   - **Purpose**: Specify how data should be read from or written to external files.\n   - **Example**:\n     ```sql\n     CREATE OR REPLACE FILE FORMAT my_format\n     TYPE = CSV\n     FIELD_OPTIONALLY_ENCLOSED_BY = ''\n     SKIP_HEADER = 1;\n     ```\n\n6. **Sequence**:\n   - **Overview**: A sequence is an object that generates unique numeric values, often used for creating surrogate keys.\n   - **Purpose**: Generate unique identifiers for rows in a table.\n   - **Example**:\n     ```sql\n     CREATE SEQUENCE my_sequence\n     START = 1\n     INCREMENT = 1;\n     ```\n\n7. **Stream**:\n   - **Overview**: A stream is a change tracking mechanism in Snowflake. It captures changes made to a table and can be used for CDC (Change Data Capture) and data replication.\n   - **Purpose**: Track and propagate changes in a table.\n   - **Example**:\n     ```sql\n     CREATE STREAM my_stream\n     ON TABLE my_table;\n     ```\n\n8. **Task**:\n   - **Overview**: A task is a scheduled or one-time execution of SQL statements. It can be used for automation, ETL, and routine maintenance.\n   - **Purpose**: Automate SQL execution on a schedule or event.\n   - **Example**:\n     ```sql\n     CREATE TASK my_task\n     WAREHOUSE = my_warehouse\n     SCHEDULE = '5 minute'\n     AS\n     INSERT INTO my_summary_table\n     SELECT ...\n     ```\n\n9. **Function** and **Procedure**:\n   - **Overview**: Functions are reusable SQL code blocks that return a value, while procedures are reusable code blocks that perform an action.\n   - **Purpose**: Encapsulate logic and promote code reuse.\n   - **Example (Function)**:\n     ```sql\n     CREATE OR REPLACE FUNCTION add_numbers(x INT, y INT)\n     RETURNS INT\n     AS\n     'x + y';\n     ```\n\n10. **Dynamic Table**:\n    - **Overview**: Dynamic tables are temporary tables created and dropped during a session. They're used for intermediate data processing.\n    - **Purpose**: Temporarily store data for complex operations without affecting permanent tables.\n    - **Example**:\n      ```sql\n      CREATE TEMPORARY TABLE temp_table AS\n      SELECT ...\n      ```\n\nThese are fundamental Snowflake concepts that cover various aspects of data storage, processing, and automation within the Snowflake data warehouse.","tags":["snowflakes"]},{"grab":"## staging using Amazon s3","views":"\n**Step-by-Step Operation:**\n\n**Step 1: Set Up Snowflake and Amazon S3**\n\nEnsure you have a Snowflake account and an Amazon S3 bucket set up. Replace placeholders in angle brackets (<>) with your actual values.\n\n**Step 2: Create a Stage in Snowflake**\n\nIn Snowflake, create an internal stage to specify the S3 bucket as the staging location:\n\n```sql\n-- Create a stage\nCREATE OR REPLACE STAGE my_stage\nURL = 's3://<your-s3-bucket>/<path-to-folder>'\nCREDENTIALS = (\n  AWS_KEY_ID = '<your-aws-access-key-id>'\n  AWS_SECRET_KEY = '<your-aws-secret-access-key>'\n);\n```\n\nThis step establishes a connection to your S3 bucket.\n\n**Step 3: Copy Data to Snowflake Stage**\n\nYou can copy data from your S3 bucket to the Snowflake stage using the `COPY INTO` command. This command specifies the source and target:\n\n```sql\n-- Copy data into Snowflake stage\nCOPY INTO my_stage\nFROM 's3://<your-s3-bucket>/<source-path>'\nFILES = ('file1.csv', 'file2.csv')\nFILE_FORMAT = (TYPE = CSV);\n```\n\nThis command copies data from the specified S3 path and files into your Snowflake stage.\n\n**Step 4: Perform Data Operations**\n\nYou can now perform various data operations on the staged data using SQL commands:\n\n```sql\n-- Example: Create a table and load data from the stage\nCREATE OR REPLACE TABLE my_table AS\nSELECT *\nFROM @my_stage/file1.csv;\n```\n\nHere, we create a new table in Snowflake and load data from the staged CSV file.\n\n**Step 5: Copy Data Back to S3 (Optional)**\n\nAfter processing the data, if you need to write the results back to S3, you can use the `COPY INTO` command again to copy data from Snowflake to S3:\n\n```sql\n-- Copy data from Snowflake to S3\nCOPY INTO 's3://<your-s3-bucket>/<destination-path>'\nFROM my_table\nFILE_FORMAT = (TYPE = CSV);\n```\n\nThis command exports data from Snowflake to the specified S3 location.\n\n**Step 6: Clean Up (Optional)**\n\nYou can optionally clean up the staged data and stage by dropping them:\n\n```sql\n-- Drop the stage\nDROP STAGE my_stage;\n\n-- Drop the table if it's no longer needed\nDROP TABLE my_table;\n```\nStaging allows you to efficiently transfer and process data between Snowflake and cloud storage services like S3 while maintaining data integrity and security.","tags":["snowflakes"]},{"grab":"## internal storage","views":"\n**Overview:**\n\n- Internal stages are automatically managed by Snowflake and don't require separate storage account credentials.\n- They are temporary and used within Snowflake for various operations like data loading, unloading, and transformations.\n- Internal stages are often used for storing intermediate data during ETL (Extract, Transform, Load) processes within Snowflake.\n\n**Creating an Internal Stage:**\n\nYou can create an internal stage using the following SQL query:\n\n```sql\n-- Create an internal Snowflake stage\nCREATE OR REPLACE STAGE my_internal_stage;\n```\n\n**Using an Internal Stage:**\n\nOnce created, you can use the internal stage in various Snowflake operations. For example, you can copy data from a Snowflake table into an internal stage or vice versa:\n\n```sql\n-- Copy data from Snowflake table to the internal stage\nCOPY INTO @my_internal_stage/<your-file-prefix>\nFROM my_snowflake_table\nFILE_FORMAT = (TYPE = CSV);\n\n-- Copy data from the internal stage to a Snowflake table\nCOPY INTO my_snowflake_table\nFROM @my_internal_stage/<your-file-prefix>\nFILE_FORMAT = (TYPE = CSV);\n```\n\nIn this example, `my_internal_stage` is the name of the internal Snowflake stage. You can use it as a source or destination for data transfer operations.\n\n**Use Cases:**\n\n- Internal stages are helpful for intermediate data storage during complex data transformations.\n- They can be used as temporary storage when processing data with Snowflake functions, such as transformations using stored procedures or tasks.\n- Internal stages are not exposed outside of Snowflake and are mainly used for managing data within the Snowflake ecosystem.\n","tags":["snowflakes"]},{"grab":"## Pipe\n\nIs use to ingest data from table to another in realtime. as a table is updated e.g stage it automatically copy the data over to the other table","views":"e.g\n\nLet's say you have an external streaming service that produces JSON data, and you want to ingest this data into a Snowflake table.\n\n```sql\n-- Create a pipe to ingest data from an external source (e.g., an S3 bucket)\nCREATE OR REPLACE PIPE my_pipe\n  AUTO_INGEST = TRUE\n  INTEGRATION = my_storage_integration\n  AS\n  COPY INTO my_snowflake_table\n  FROM @my_external_stage\n  FILE_FORMAT = (TYPE = JSON);\n\n```\n\nfor Internal stage:\n\n```sql\n-- Create an internal stage (if it doesn't exist)\nCREATE OR REPLACE STAGE my_internal_stage;\n\n-- Create a pipe to ingest data from an internal stage\nCREATE OR REPLACE PIPE my_pipe_internal_stage\n  AUTO_INGEST = TRUE\n  AS\n  COPY INTO my_snowflake_table\n  FROM @my_internal_stage\n  FILE_FORMAT = (TYPE = JSON);\n\n```","tags":["snowflakes"]},{"grab":"## Notification\n\nsnowflakes can send notification for data ingestion and the likes. this helps to monitor services","views":"```sql\n-- Define a notification integration for email notifications\nCREATE NOTIFICATION INTEGRATION my_email_integration\n  TYPE = EMAIL\n  NOTIFICATION_PROVIDER = 'smtp'\n  ENABLED = TRUE\n  EMAIL_SUBJECT = 'Data Loading Notification'\n  EMAIL_SENDER_ADDRESS = 'your_email@example.com'\n  EMAIL_RECIPIENT_ADDRESS = 'recipient@example.com';\n\n-- Associate the notification integration with a pipe\nCREATE OR REPLACE PIPE my_pipe\n  AUTO_INGEST = TRUE\n  INTEGRATION = my_email_integration\n  ...other pipe settings...;\n\n```\n\nThe type can either be `EMAIL | QUEUE`","tags":["snowflakes"]},{"grab":"## Storage Integration\n\nGood for securly managing external data wrt snoweflakes.\n\nE.g if we are to move data from  amazon s3 to snowflakes and want to mange that via staging.\n\nwe can create a storage integration with credentials set and no need to always set the credentials incase we are to create another staging","views":"1. Define a storage integration:\n```sql\n-- Create a Storage Integration for Amazon S3\nCREATE STORAGE INTEGRATION my_s3_integration\n  TYPE = EXTERNAL_STAGE\n  ENABLED = TRUE\n  STORAGE_PROVIDER = S3\n  STORAGE_AWS_ROLE_ARN = 'arn:aws:iam::123456789012:role/SnowflakeRole'\n  STORAGE_AWS_EXTERNAL_ID = 'snowflake-external-id';\n\n```\n\n2. Access control:\n```sql\n-- Grant USAGE privilege on the Storage Integration\nGRANT USAGE ON INTEGRATION my_s3_integration TO ROLE my_role;\n```\n\n3. Using a storage integration:\n```sql\n-- Create an external stage using the Storage Integration\nCREATE OR REPLACE STAGE my_external_stage\n  URL = 's3://my-bucket/data/'\n  STORAGE_INTEGRATION = my_s3_integration;\n\n-- Copy data from the external stage to a Snowflake table\nCOPY INTO my_snowflake_table\n  FROM @my_external_stage/data\n  FILES = ('file1.csv', 'file2.csv');\n\n```","tags":["snowflakes"]},{"grab":"## File format\n\nis use to determin how file is read into snowflakes or copied out of snowflakes\n\n","views":"```sql\nCREATE OR REPLACE FILE FORMAT my_csv_format\nTYPE = 'CSV'\nFIELD_OPTIONALLY_ENCLOSED_BY = ''\nSKIP_HEADER = 1;\n```","tags":["snowflakes"]},{"grab":"## sequence\n\nUse to create sequential unique numbers","views":"```sql\nCREATE OR REPLACE SEQUENCE seq_01 START = 1 INCREMENT = 1;\nCREATE OR REPLACE TABLE sequence_test_table (i INTEGER);\n```\n\nrun the select query on seq_01 to generate sequential numbers\n\n```sql\nSELECT seq_01.nextval;\n+---------+\n| NEXTVAL |\n|---------|\n|       1 |\n+---------+\n```\n2nd time\n\n```sql\nSELECT seq_01.nextval;\n+---------+\n| NEXTVAL |\n|---------|\n|       2 |\n+---------+\n```\n\ninsert the dequence number into a table\n\n```sql\nINSERT INTO sequence_test_table (i) VALUES (seq_01.nextval);\n\nSELECT i FROM sequence_test_table;\n+---+\n| I |\n|---|\n| 3 |\n+---+\n```\n ","tags":["snowflakes"]},{"grab":"## stream\n\nAllows us to capture and track changes (real time) to tables.","views":"```sql\nCREATE STREAM my_stream ON TABLE my_table;\n\n```\n\ntracks every changes to table `my_table`\n\n\nto see the streams \n\n```sql\nselect * from my_stream\n\n+----+--------+-----+-----------------+-------------------+------------------------------------------+\n| ID | NAME   | FEE | METADATA$ACTION | METADATA$ISUPDATE | METADATA$ROW_ID                          |\n|----+--------+-----+-----------------+-------------------+------------------------------------------|\n|  1 | Joe    |  90 | INSERT          | False             | 957e84b34ef0f3d957470e02bddccb027810892c |\n|  2 | Jane   |  90 | INSERT          | False             | b00168a4edb9fb399dd5cc015e5f78cbea158956 |\n|  3 | George |  90 | INSERT          | False             | 75206259362a7c89126b7cb039371a39d821f76a |\n|  4 | Betty  |   0 | INSERT          | False             | 9b225bc2612d5e57b775feea01dd04a32ce2ad18 |\n|  5 | Sally  |   0 | INSERT          | False             | 5a68f6296c975980fbbc569ce01033c192168eca |\n+----+--------+-----+-----------------+-------------------+------------------------------------------+\n\n```","tags":["snowflakes"]}]